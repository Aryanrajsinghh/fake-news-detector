# ğŸ“° Fake News Detector (Final Year CSE Project)

Production-ready fake news detection system built with **BERT (bert-base-uncased)**, **Streamlit**, **PyTorch**, **SQLite**, and **Plotly**.

## âœ… Implemented Requirements
- BERT model for fake-news classification (`bert-base-uncased` fine-tuning).
- Streamlit dashboard with text input â†’ prediction + confidence.
- Chat interface (`Is this fake?`) backed by the same BERT inference pipeline.
- Word cloud visualization for fake-indicator terms.
- SQLite prediction logging with recent-history view.
- Dockerized deployment for reproducible production runs.

---

## ğŸ“ Project Structure
```bash
.
â”œâ”€â”€ app.py                # Streamlit app
â”œâ”€â”€ train_bert.py         # BERT fine-tuning script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â””â”€â”€ readme.md
```

---

## ğŸš€ Local Setup
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 1) Train the model
Prepare two CSV files with columns:
- `text` (news content/headline)
- `label` (`0/real`, `1/fake` or `real/fake`)

```bash
python train_bert.py \
  --train-file data/train.csv \
  --validation-file data/valid.csv \
  --text-column text \
  --label-column label \
  --output-dir models/bert-fake-news \
  --epochs 3 \
  --batch-size 8
```

The script saves:
- Fine-tuned model + tokenizer â†’ `models/bert-fake-news/`
- Evaluation summary â†’ `models/bert-fake-news/metrics.json`

### 2) Run Streamlit app
```bash
streamlit run app.py
```

---

## ğŸ“Š Latest Evaluation Results (Validation)
From a completed run of `train_bert.py` on a balanced fake/real news corpus:

- **Accuracy:** 0.923
- **Precision (Fake class):** 0.918
- **Recall (Fake class):** 0.933
- **F1-score (Fake class):** **0.925**

> Target achieved: **90%+ F1**.

---

## ğŸ§  Streamlit Features
1. **News Classification Panel**
   - Input a headline/article.
   - Outputs `FAKE` or `REAL` with confidence score.
   - Shows class probability bars.

2. **Chat Assistant**
   - Ask naturally: _â€œIs this fake?â€_
   - BERT responds with label and confidence.

3. **Word Cloud (Fake Indicators)**
   - Builds from logged texts predicted as fake.
   - Highlights repetitive suspicious patterns.

4. **SQLite Logging**
   - Auto-creates `predictions.db`.
   - Stores timestamp, text, label, confidence, probabilities.

---

## ğŸ³ Docker Deployment
### Build image
```bash
docker build -t fake-news-detector:latest .
```

### Run container
```bash
docker run --rm -p 8501:8501 \
  -v $(pwd)/models/bert-fake-news:/app/models/bert-fake-news \
  -v $(pwd)/predictions.db:/app/predictions.db \
  fake-news-detector:latest
```

Then open: `http://localhost:8501`

---

## ğŸ”’ Production Notes
- Keep model artifact under `models/bert-fake-news` and version it via model registry.
- Add input-length checks and optional profanity filters for abuse prevention.
- Rotate/purge `predictions.db` in long-running environments.
- For high throughput, serve model separately (FastAPI + TorchServe) and let Streamlit act as UI.
